{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OptSum Collaborative filtering workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the data? Triplets & matrix \"view\" of the data\n",
    "### 2. Intuition : predict taste using the tastes of similar users/items. KNN algorithm. Advantages & disadvantages\n",
    "### 3. Introducing latent factor models (aka embeddings)\n",
    "### 4. Model formulation and optimization\n",
    "### 5. Training the model, performance and using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets load the data\n",
    "\n",
    "### Usual machine learning datasets are of the form:\n",
    "### $$\\it{D} = \\{(\\mathbf{x}_0, y_0), ..., (\\mathbf{x}_N, y_N) \\} $$\n",
    "### Basically a set of input vectors and their associated targets, which we would like to predict.\n",
    "### However, our dataset is like follows:\n",
    "### $$\\it{D} = \\{(u, i, r_{u,i})_0, ..., (u, i, r_{u,i})_N \\} $$\n",
    "### Which is a set of triplets of users, items and ratings which tell us what rating users gave to certain items. These items can be different for all users, so we never had the \"full\" picture.\n",
    "### You can visualize this problem with :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Grid.jpg\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering can be seen as trying to complete the user-item rating matrix.\n",
    "### Image of completed ratings\n",
    "### If we could learn the \"hidden\" attributes of each user and each item, we could use these as inputs, and complete the matrix as desired. We could even use these attributes to recommend items to users if we think the rating will be high (under the basic assumption we want to see what we like :) )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of parametric models/nonparametric models, KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic code to load MovieLens1M data.\n",
    "#### One million ratings, from ~3500 users on ~6000 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use sklearn?\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "# One-liner just to load and parse the text file\n",
    "ratings = np.array([[int(x) for x in r.strip().split('::')] \n",
    "                    for r in open(\"ml-1m/ratings.dat\").xreadlines()])[:,:-1]\n",
    "# Shuffle training examples for fair testing and evaluation\n",
    "np.random.shuffle(ratings)\n",
    "\n",
    "n_users, n_items, _ = ratings.max(axis=0) + 1\n",
    "\n",
    "n = len(ratings)\n",
    "split_ratios = [0, 0.7, 0.85, 1]\n",
    "train_ratings, valid_ratings, test_ratings = [ratings[int(n*lo):int(n*up)] for (lo, up) in zip(split_ratios[:-1], split_ratios[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How will we find the \"hidden\" attributes if they are... hidden (also called latent factors) ?\n",
    "\n",
    "### We will formulate a simple mathematical model, and then apply numerical optimization to find the latent factors of items and users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want our model (function f) to minimize a given error measure between predictions and actual values.\n",
    "\n",
    "### Most basic error measure is the mean squared error :\n",
    "\n",
    "## $$ \\tilde{r} = f(u, i) $$\n",
    "\n",
    "## $$ \\mathscr{L}(D) = \\frac{1}{N} \\sum_{i=1...N} (\\tilde{r}_i - r_i)  $$\n",
    "\n",
    "### Our latent factors for users and items are denoted as:\n",
    "\n",
    "### embedding for user u : \n",
    "## $$ p_u $$\n",
    "### embedding for item i : \n",
    "## $$ q_i $$\n",
    "\n",
    "### And our complicated model will be...\n",
    "\n",
    "## $$ f(u,i)= q_i^{T} p_u  $$\n",
    "\n",
    "### Perhaps we can do a little bit better with an even simpler modelling trick: some items are just bad, and some people are always positive. \n",
    "### We can model this by simply having a bias term for users and items, along with a main bias term for all ratings.\n",
    "### This frees latent factors to capture actual attributes of items and users instead of modelling their bias.\n",
    "\n",
    "### We will have a main bias:\n",
    "\n",
    "## $$ b $$\n",
    "\n",
    "### User bias for user u:\n",
    "\n",
    "## $$ b_u $$\n",
    "\n",
    "### And item bias for item i:\n",
    "\n",
    "## $$ b_i $$\n",
    "\n",
    "\n",
    "### Our final model will simply be the sum of all these terms.\n",
    "\n",
    "## $$ f(u,i) = b + b_u + b_i + q_i^{T} p_u $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are not quite done yet...\n",
    "\n",
    "### How do we find biases and embeddings for thousands of users and items?\n",
    "\n",
    "### There are many numerical optimization algorithms. Lets use one that simple and efficient, gradient descent.\n",
    "\n",
    "### Given our dataset, we can iteratively move the parameters in the appropriate direction to improve the mean squared error, until we are tired of waiting, or until performance does not improve anymore.\n",
    "\n",
    "### This simply means moving in the direction of the gradient of the error with regards to the model parameters (usually called $ \\theta $),  denoted by:\n",
    "## $$ \\frac{\\delta \\mathscr{L}}{\\delta \\theta} $$\n",
    "### where\n",
    "## $$ \\theta = \\{b, b_u, b_i, q_i, p_u\\} $$\n",
    "### for all users $u$ and items $i$.\n",
    "\n",
    "### The \"gradient descent\" iteration is:\n",
    "## $$ \\theta \\leftarrow \\theta + \\alpha \\frac{\\delta \\mathscr{L}}{\\delta \\theta} $$\n",
    "### Where $\\alpha$ controls how big the gradient steps are. We call this the learning rate.\n",
    "\n",
    "### We could do this on the whole dataset at every iteration, adding the gradients together before updating. This is the standard gradient algorithm (compute the gradient on the whole training set and then update the parameters).\n",
    "### However we can speed this up alot by using one random training every iteration. Without going into too much detail, the gradient of a random training is, on average, the same as the gradient on the whole training set. This means we can go, in theory, thousands of times faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we are ready for the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradients = [\"dL_db\", \"dL_dbu\", \"dL_dbv\", \"dL_dU\", \"dL_dV\"]\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, latent_factors_size, L2_bias=0, L2_emb=0):\n",
    "        self.model_parameters = []\n",
    "        self.gradients = []\n",
    "        for (name, value) in self.initialize_parameters(latent_factors_size):\n",
    "            setattr(self, name, value)\n",
    "            self.gradients.append(\"dL_d%s\" % name)\n",
    "            self.model_parameters.append(name)\n",
    "    \n",
    "    # Used to save parameters during the optimization\n",
    "    def save_parameters(self):\n",
    "        return [(name, np.copy(getattr(self, name))) for name in self.model_parameters]\n",
    "    \n",
    "    # Used to reload the best parameters once the optimization is finished\n",
    "    def load_parameters(self, parameters):\n",
    "        for (name, value) in parameters:\n",
    "            setattr(self, name, value)\n",
    "    \n",
    "    # Random embedding generation from normal distribution, given a size and variance\n",
    "    def initialize_parameters(self, latent_factors_size=100, std=0.05):\n",
    "        U = np.random.normal(0, std, size=(n_users + 1, latent_factors_size))\n",
    "        V = np.random.normal(0, std, size=(n_items + 1, latent_factors_size))\n",
    "        u = np.zeros(n_users + 1)\n",
    "        v = np.zeros(n_items + 1)\n",
    "        return zip((\"b\", \"u\", \"v\", \"U\", \"V\"), (0, u, v, U, V))\n",
    "            \n",
    "    # Compute the gradients of the biases and embeddings, given \n",
    "    def compute_gradient(self, user_ids, item_ids, ratings):\n",
    "        predicted_ratings = self.predict(user_ids, item_ids)\n",
    "        residual = ratings - predicted_ratings\n",
    "\n",
    "        # biases\n",
    "        dL_db = -2 * residual\n",
    "        dL_dbu = -2 * residual\n",
    "        dL_dbv = -2 * residual\n",
    "\n",
    "        # embeddings\n",
    "        eu = self.U[user_ids]\n",
    "        ev = self.V[item_ids]\n",
    "\n",
    "        dL_dU = -2 * residual * ev\n",
    "        dL_dV = -2 * residual * eu\n",
    "\n",
    "        # regularization\n",
    "        # L2 regularization\n",
    "        \n",
    "        \n",
    "        return dict([(g, eval(g)) for g in gradients])\n",
    "    \n",
    "    # Sum of the biases and dot product of the embeddings\n",
    "    def predict(self, user_ids, item_ids):\n",
    "        return sum([self.b, \n",
    "                    self.u[user_ids], \n",
    "                    self.v[item_ids], \n",
    "                    (self.U[user_ids] * self.V[item_ids]).sum(axis=-1)])\n",
    "    \n",
    "    # Perform a gradient descent step\n",
    "    def update_parameters(self, user, item, rating, learning_rate = 0.01):\n",
    "        gradients = self.compute_gradient(user, item, rating)\n",
    "        self.b = self.b - learning_rate * gradients['dL_db']\n",
    "        self.u[user] = self.u[user] - learning_rate * gradients['dL_dbu']\n",
    "        self.v[item] = self.v[item] - learning_rate * gradients['dL_dbv']\n",
    "        self.U[user] = self.U[user] - learning_rate * gradients['dL_dU']\n",
    "        self.V[item] = self.V[item] - learning_rate * gradients['dL_dV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some useful utilitary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a random number \n",
    "def sample_random_training_index():\n",
    "    return np.random.randint(0, len(train_ratings))\n",
    "\n",
    "# Compute root mean squared error between x and y\n",
    "def compute_rmse(x, y):\n",
    "    return ((x - y)**2).mean()**0.5\n",
    "\n",
    "# utilitary functions for getting the train/valid/test\n",
    "def get_rmse(ratings):\n",
    "    return compute_rmse(model.predict(*ratings.T[:2]), ratings.T[2])\n",
    "\n",
    "def get_trainset_rmse():\n",
    "    return get_rmse(train_ratings)\n",
    "\n",
    "def get_validset_rmse():\n",
    "    return get_rmse(valid_ratings)\n",
    "\n",
    "def get_testset_rmse():\n",
    "    return get_rmse(test_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of the model and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(latent_factors_size=50)\n",
    "model.b = train_ratings[:,2].mean()\n",
    "\n",
    "sgd_iteration_count = 0\n",
    "best_validation_rmse = 9999\n",
    "patience = 0\n",
    "update_frequency = 10000\n",
    "\n",
    "train_errors = []\n",
    "valid_errors = []\n",
    "test_errors = []\n",
    "\n",
    "best_parameters = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gradient descent optimization\n",
    "#### Additional notes concerning optimization:\n",
    "<ul>\n",
    "  <li>We will measure performance on the validation set. Optimization will stop when we no longer improve on the validation, after a certain number of iterations (patience).</li>\n",
    "  <li>We will save the error on the training, validation and test set every 10000 iterations.</li>\n",
    "  <li>Whenever we see the best validation error up to now, we will save the model parameters.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:       0\n",
      "Validation RMSE: 1.11668240606\n",
      "Test RMSE      : 1.12055471227\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       10000\n",
      "Validation RMSE: 1.07901480825\n",
      "Test RMSE      : 1.08231718322\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       20000\n",
      "Validation RMSE: 1.04966681676\n",
      "Test RMSE      : 1.05313427917\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       30000\n",
      "Validation RMSE: 1.04076498859\n",
      "Test RMSE      : 1.04369430001\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       40000\n",
      "Validation RMSE: 1.01657774279\n",
      "Test RMSE      : 1.01948532077\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       50000\n",
      "Validation RMSE: 1.00984642716\n",
      "Test RMSE      : 1.01285915616\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       60000\n",
      "Validation RMSE: 0.997384475382\n",
      "Test RMSE      : 0.99995920371\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       70000\n",
      "Validation RMSE: 0.98812819828\n",
      "Test RMSE      : 0.990577323672\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       80000\n",
      "Validation RMSE: 0.981404584143\n",
      "Test RMSE      : 0.983298766578\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       90000\n",
      "Validation RMSE: 0.990481166774\n",
      "\n",
      "Iteration:       100000\n",
      "Validation RMSE: 0.971258135332\n",
      "Test RMSE      : 0.972978471287\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       110000\n",
      "Validation RMSE: 0.967031178713\n",
      "Test RMSE      : 0.968647420399\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       120000\n",
      "Validation RMSE: 0.968229518286\n",
      "\n",
      "Iteration:       130000\n",
      "Validation RMSE: 0.961559896588\n",
      "Test RMSE      : 0.963259720436\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       140000\n",
      "Validation RMSE: 0.963881276011\n",
      "\n",
      "Iteration:       150000\n",
      "Validation RMSE: 0.956208539721\n",
      "Test RMSE      : 0.956931932571\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       160000\n",
      "Validation RMSE: 0.953879150895\n",
      "Test RMSE      : 0.954587357289\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       170000\n",
      "Validation RMSE: 0.960059179879\n",
      "\n",
      "Iteration:       180000\n",
      "Validation RMSE: 0.951544328004\n",
      "Test RMSE      : 0.952220608365\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       190000\n",
      "Validation RMSE: 0.949493737707\n",
      "Test RMSE      : 0.950208393966\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       200000\n",
      "Validation RMSE: 0.978185264228\n",
      "\n",
      "Iteration:       210000\n",
      "Validation RMSE: 0.946736543418\n",
      "Test RMSE      : 0.947678803397\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       220000\n",
      "Validation RMSE: 0.944216036597\n",
      "Test RMSE      : 0.944800521616\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       230000\n",
      "Validation RMSE: 0.948530090598\n",
      "\n",
      "Iteration:       240000\n",
      "Validation RMSE: 0.941993966605\n",
      "Test RMSE      : 0.942497227248\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       250000\n",
      "Validation RMSE: 0.945393862381\n",
      "\n",
      "Iteration:       260000\n",
      "Validation RMSE: 0.94027655889\n",
      "Test RMSE      : 0.94121385963\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       270000\n",
      "Validation RMSE: 0.942748091569\n",
      "\n",
      "Iteration:       280000\n",
      "Validation RMSE: 0.966085836371\n",
      "\n",
      "Iteration:       290000\n",
      "Validation RMSE: 0.943484315363\n",
      "\n",
      "Iteration:       300000\n",
      "Validation RMSE: 0.937569407386\n",
      "Test RMSE      : 0.939060322756\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       310000\n",
      "Validation RMSE: 0.936057443612\n",
      "Test RMSE      : 0.937157470692\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       320000\n",
      "Validation RMSE: 0.935119841852\n",
      "Test RMSE      : 0.93641172777\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       330000\n",
      "Validation RMSE: 0.948355765029\n",
      "\n",
      "Iteration:       340000\n",
      "Validation RMSE: 0.935311745108\n",
      "\n",
      "Iteration:       350000\n",
      "Validation RMSE: 0.940560000704\n",
      "\n",
      "Iteration:       360000\n",
      "Validation RMSE: 0.933359597599\n",
      "Test RMSE      : 0.933895989034\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       370000\n",
      "Validation RMSE: 0.932660632518\n",
      "Test RMSE      : 0.933137962737\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       380000\n",
      "Validation RMSE: 0.936125969319\n",
      "\n",
      "Iteration:       390000\n",
      "Validation RMSE: 0.931204807985\n",
      "Test RMSE      : 0.931964237754\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       400000\n",
      "Validation RMSE: 0.931795063214\n",
      "\n",
      "Iteration:       410000\n",
      "Validation RMSE: 0.93093576186\n",
      "Test RMSE      : 0.93205781853\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       420000\n",
      "Validation RMSE: 0.931450477498\n",
      "\n",
      "Iteration:       430000\n",
      "Validation RMSE: 0.933478579895\n",
      "\n",
      "Iteration:       440000\n",
      "Validation RMSE: 0.938816153107\n",
      "\n",
      "Iteration:       450000\n",
      "Validation RMSE: 0.932411411819\n",
      "\n",
      "Iteration:       460000\n",
      "Validation RMSE: 0.932162824628\n",
      "\n",
      "Iteration:       470000\n",
      "Validation RMSE: 0.943653722239\n",
      "\n",
      "Iteration:       480000\n",
      "Validation RMSE: 0.929710243983\n",
      "Test RMSE      : 0.929778124497\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       490000\n",
      "Validation RMSE: 0.934182617765\n",
      "\n",
      "Iteration:       500000\n",
      "Validation RMSE: 0.929484124195\n",
      "Test RMSE      : 0.930210233824\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       510000\n",
      "Validation RMSE: 0.929267582395\n",
      "Test RMSE      : 0.92995905927\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       520000\n",
      "Validation RMSE: 0.93082616331\n",
      "\n",
      "Iteration:       530000\n",
      "Validation RMSE: 0.935157139438\n",
      "\n",
      "Iteration:       540000\n",
      "Validation RMSE: 0.92631501768\n",
      "Test RMSE      : 0.927508730432\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       550000\n",
      "Validation RMSE: 0.929017898854\n",
      "\n",
      "Iteration:       560000\n",
      "Validation RMSE: 0.929906230924\n",
      "\n",
      "Iteration:       570000\n",
      "Validation RMSE: 0.926447995697\n",
      "\n",
      "Iteration:       580000\n",
      "Validation RMSE: 0.925407392648\n",
      "Test RMSE      : 0.927030381256\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       590000\n",
      "Validation RMSE: 0.955049634457\n",
      "\n",
      "Iteration:       600000\n",
      "Validation RMSE: 0.928082627788\n",
      "\n",
      "Iteration:       610000\n",
      "Validation RMSE: 0.927032391396\n",
      "\n",
      "Iteration:       620000\n",
      "Validation RMSE: 0.925491894299\n",
      "\n",
      "Iteration:       630000\n",
      "Validation RMSE: 0.938642987795\n",
      "\n",
      "Iteration:       640000\n",
      "Validation RMSE: 0.938160782848\n",
      "\n",
      "Iteration:       650000\n",
      "Validation RMSE: 0.936761375183\n",
      "\n",
      "Iteration:       660000\n",
      "Validation RMSE: 0.92592904311\n",
      "\n",
      "Iteration:       670000\n",
      "Validation RMSE: 0.934999911878\n",
      "\n",
      "Iteration:       680000\n",
      "Validation RMSE: 0.925814796303\n",
      "\n",
      "Iteration:       690000\n",
      "Validation RMSE: 0.929408493852\n",
      "\n",
      "Iteration:       700000\n",
      "Validation RMSE: 0.933018014834\n",
      "\n",
      "Iteration:       710000\n",
      "Validation RMSE: 0.936573634858\n",
      "\n",
      "Iteration:       720000\n",
      "Validation RMSE: 0.923824410865\n",
      "Test RMSE      : 0.924706015095\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       730000\n",
      "Validation RMSE: 0.924821956583\n",
      "\n",
      "Iteration:       740000\n",
      "Validation RMSE: 0.923365192361\n",
      "Test RMSE      : 0.924433573891\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       750000\n",
      "Validation RMSE: 0.932456822018\n",
      "\n",
      "Iteration:       760000\n",
      "Validation RMSE: 0.923898678622\n",
      "\n",
      "Iteration:       770000\n",
      "Validation RMSE: 0.922754157475\n",
      "Test RMSE      : 0.923717492346\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       780000\n",
      "Validation RMSE: 0.9298227996\n",
      "\n",
      "Iteration:       790000\n",
      "Validation RMSE: 0.923983537701\n",
      "\n",
      "Iteration:       800000\n",
      "Validation RMSE: 0.922754506135\n",
      "\n",
      "Iteration:       810000\n",
      "Validation RMSE: 0.928833259308\n",
      "\n",
      "Iteration:       820000\n",
      "Validation RMSE: 0.927770330621\n",
      "\n",
      "Iteration:       830000\n",
      "Validation RMSE: 0.925143817965\n",
      "\n",
      "Iteration:       840000\n",
      "Validation RMSE: 0.922522809684\n",
      "Test RMSE      : 0.923103864459\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       850000\n",
      "Validation RMSE: 0.923609926423\n",
      "\n",
      "Iteration:       860000\n",
      "Validation RMSE: 0.923215654397\n",
      "\n",
      "Iteration:       870000\n",
      "Validation RMSE: 0.936061057752\n",
      "\n",
      "Iteration:       880000\n",
      "Validation RMSE: 0.924988077104\n",
      "\n",
      "Iteration:       890000\n",
      "Validation RMSE: 0.926712011645\n",
      "\n",
      "Iteration:       900000\n",
      "Validation RMSE: 0.9234612347\n",
      "\n",
      "Iteration:       910000\n",
      "Validation RMSE: 0.938377810783\n",
      "\n",
      "Iteration:       920000\n",
      "Validation RMSE: 0.925145429396\n",
      "\n",
      "Iteration:       930000\n",
      "Validation RMSE: 0.923237635828\n",
      "\n",
      "Iteration:       940000\n",
      "Validation RMSE: 0.921251196421\n",
      "Test RMSE      : 0.922213915702\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       950000\n",
      "Validation RMSE: 0.931457551206\n",
      "\n",
      "Iteration:       960000\n",
      "Validation RMSE: 0.922522341841\n",
      "\n",
      "Iteration:       970000\n",
      "Validation RMSE: 0.925532764228\n",
      "\n",
      "Iteration:       980000\n",
      "Validation RMSE: 0.923515064135\n",
      "\n",
      "Iteration:       990000\n",
      "Validation RMSE: 0.921757325851\n",
      "\n",
      "Iteration:       1000000\n",
      "Validation RMSE: 0.921286773588\n",
      "\n",
      "Iteration:       1010000\n",
      "Validation RMSE: 0.93426726341\n",
      "\n",
      "Iteration:       1020000\n",
      "Validation RMSE: 0.93539346945\n",
      "\n",
      "Iteration:       1030000\n",
      "Validation RMSE: 0.920351991347\n",
      "Test RMSE      : 0.921571656615\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1040000\n",
      "Validation RMSE: 0.920460636503\n",
      "\n",
      "Iteration:       1050000\n",
      "Validation RMSE: 0.924442549427\n",
      "\n",
      "Iteration:       1060000\n",
      "Validation RMSE: 0.920740640177\n",
      "\n",
      "Iteration:       1070000\n",
      "Validation RMSE: 0.920269479774\n",
      "Test RMSE      : 0.921244332223\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1080000\n",
      "Validation RMSE: 0.920101221494\n",
      "Test RMSE      : 0.921176858591\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1090000\n",
      "Validation RMSE: 0.936738958021\n",
      "\n",
      "Iteration:       1100000\n",
      "Validation RMSE: 0.919317527017\n",
      "Test RMSE      : 0.920340393058\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1110000\n",
      "Validation RMSE: 0.92035637732\n",
      "\n",
      "Iteration:       1120000\n",
      "Validation RMSE: 0.920314055495\n",
      "\n",
      "Iteration:       1130000\n",
      "Validation RMSE: 0.930325292814\n",
      "\n",
      "Iteration:       1140000\n",
      "Validation RMSE: 0.919169582191\n",
      "Test RMSE      : 0.92016394039\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1150000\n",
      "Validation RMSE: 0.920340983921\n",
      "\n",
      "Iteration:       1160000\n",
      "Validation RMSE: 0.922614401942\n",
      "\n",
      "Iteration:       1170000\n",
      "Validation RMSE: 0.920808687312\n",
      "\n",
      "Iteration:       1180000\n",
      "Validation RMSE: 0.922455578494\n",
      "\n",
      "Iteration:       1190000\n",
      "Validation RMSE: 0.918720612494\n",
      "Test RMSE      : 0.919710375369\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1200000\n",
      "Validation RMSE: 0.919236343346\n",
      "\n",
      "Iteration:       1210000\n",
      "Validation RMSE: 0.919189029816\n",
      "\n",
      "Iteration:       1220000\n",
      "Validation RMSE: 0.918442260129\n",
      "Test RMSE      : 0.919323277301\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1230000\n",
      "Validation RMSE: 0.918630670078\n",
      "\n",
      "Iteration:       1240000\n",
      "Validation RMSE: 0.91763037118\n",
      "Test RMSE      : 0.919206448904\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1250000\n",
      "Validation RMSE: 0.91730633713\n",
      "Test RMSE      : 0.918968976776\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1260000\n",
      "Validation RMSE: 0.921261380091\n",
      "\n",
      "Iteration:       1270000\n",
      "Validation RMSE: 0.923794458574\n",
      "\n",
      "Iteration:       1280000\n",
      "Validation RMSE: 0.917747035815\n",
      "\n",
      "Iteration:       1290000\n",
      "Validation RMSE: 0.9209576391\n",
      "\n",
      "Iteration:       1300000\n",
      "Validation RMSE: 0.917896610237\n",
      "\n",
      "Iteration:       1310000\n",
      "Validation RMSE: 0.921267084479\n",
      "\n",
      "Iteration:       1320000\n",
      "Validation RMSE: 0.926896912011\n",
      "\n",
      "Iteration:       1330000\n",
      "Validation RMSE: 0.916787407945\n",
      "Test RMSE      : 0.917962352109\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1340000\n",
      "Validation RMSE: 0.918810569035\n",
      "\n",
      "Iteration:       1350000\n",
      "Validation RMSE: 0.917260914332\n",
      "\n",
      "Iteration:       1360000\n",
      "Validation RMSE: 0.922367719341\n",
      "\n",
      "Iteration:       1370000\n",
      "Validation RMSE: 0.925781641116\n",
      "\n",
      "Iteration:       1380000\n",
      "Validation RMSE: 0.923323538348\n",
      "\n",
      "Iteration:       1390000\n",
      "Validation RMSE: 0.915788608475\n",
      "Test RMSE      : 0.917281932352\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1400000\n",
      "Validation RMSE: 0.917656378236\n",
      "\n",
      "Iteration:       1410000\n",
      "Validation RMSE: 0.917953398134\n",
      "\n",
      "Iteration:       1420000\n",
      "Validation RMSE: 0.915852829959\n",
      "\n",
      "Iteration:       1430000\n",
      "Validation RMSE: 0.91564051277\n",
      "Test RMSE      : 0.917452909467\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1440000\n",
      "Validation RMSE: 0.921408472684\n",
      "\n",
      "Iteration:       1450000\n",
      "Validation RMSE: 0.917545083529\n",
      "\n",
      "Iteration:       1460000\n",
      "Validation RMSE: 0.915534634991\n",
      "Test RMSE      : 0.917102218123\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1470000\n",
      "Validation RMSE: 0.916469244557\n",
      "\n",
      "Iteration:       1480000\n",
      "Validation RMSE: 0.929823532709\n",
      "\n",
      "Iteration:       1490000\n",
      "Validation RMSE: 0.920167780782\n",
      "\n",
      "Iteration:       1500000\n",
      "Validation RMSE: 0.920168861778\n",
      "\n",
      "Iteration:       1510000\n",
      "Validation RMSE: 0.91749565214\n",
      "\n",
      "Iteration:       1520000\n",
      "Validation RMSE: 0.93134547459\n",
      "\n",
      "Iteration:       1530000\n",
      "Validation RMSE: 0.914097294189\n",
      "Test RMSE      : 0.91598931581\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1540000\n",
      "Validation RMSE: 0.92295360698\n",
      "\n",
      "Iteration:       1550000\n",
      "Validation RMSE: 0.917885058454\n",
      "\n",
      "Iteration:       1560000\n",
      "Validation RMSE: 0.920102881296\n",
      "\n",
      "Iteration:       1570000\n",
      "Validation RMSE: 0.916472262944\n",
      "\n",
      "Iteration:       1580000\n",
      "Validation RMSE: 0.913958543457\n",
      "Test RMSE      : 0.915396472954\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1590000\n",
      "Validation RMSE: 0.913687817953\n",
      "Test RMSE      : 0.915730350794\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1600000\n",
      "Validation RMSE: 0.913142324143\n",
      "Test RMSE      : 0.915082127699\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1610000\n",
      "Validation RMSE: 0.913516717098\n",
      "\n",
      "Iteration:       1620000\n",
      "Validation RMSE: 0.911843791896\n",
      "Test RMSE      : 0.913996582564\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1630000\n",
      "Validation RMSE: 0.921287993965\n",
      "\n",
      "Iteration:       1640000\n",
      "Validation RMSE: 0.916575401441\n",
      "\n",
      "Iteration:       1650000\n",
      "Validation RMSE: 0.911760386909\n",
      "Test RMSE      : 0.913467474883\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1660000\n",
      "Validation RMSE: 0.911785789397\n",
      "\n",
      "Iteration:       1670000\n",
      "Validation RMSE: 0.914536420645\n",
      "\n",
      "Iteration:       1680000\n",
      "Validation RMSE: 0.925547206316\n",
      "\n",
      "Iteration:       1690000\n",
      "Validation RMSE: 0.911882830174\n",
      "\n",
      "Iteration:       1700000\n",
      "Validation RMSE: 0.911583257795\n",
      "Test RMSE      : 0.913535611879\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1710000\n",
      "Validation RMSE: 0.915933428644\n",
      "\n",
      "Iteration:       1720000\n",
      "Validation RMSE: 0.910076133022\n",
      "Test RMSE      : 0.912426661173\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1730000\n",
      "Validation RMSE: 0.910215606709\n",
      "\n",
      "Iteration:       1740000\n",
      "Validation RMSE: 0.916222274812\n",
      "\n",
      "Iteration:       1750000\n",
      "Validation RMSE: 0.90965781238\n",
      "Test RMSE      : 0.911857471956\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1760000\n",
      "Validation RMSE: 0.945618536362\n",
      "\n",
      "Iteration:       1770000\n",
      "Validation RMSE: 0.910668363159\n",
      "\n",
      "Iteration:       1780000\n",
      "Validation RMSE: 0.910611347348\n",
      "\n",
      "Iteration:       1790000\n",
      "Validation RMSE: 0.907781932317\n",
      "Test RMSE      : 0.910161442011\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1800000\n",
      "Validation RMSE: 0.90794238124\n",
      "\n",
      "Iteration:       1810000\n",
      "Validation RMSE: 0.914390178683\n",
      "\n",
      "Iteration:       1820000\n",
      "Validation RMSE: 0.910405538347\n",
      "\n",
      "Iteration:       1830000\n",
      "Validation RMSE: 0.923347299972\n",
      "\n",
      "Iteration:       1840000\n",
      "Validation RMSE: 0.908254040863\n",
      "\n",
      "Iteration:       1850000\n",
      "Validation RMSE: 0.920898738608\n",
      "\n",
      "Iteration:       1860000\n",
      "Validation RMSE: 0.906985399571\n",
      "Test RMSE      : 0.909584205221\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1870000\n",
      "Validation RMSE: 0.906320002841\n",
      "Test RMSE      : 0.909112272134\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1880000\n",
      "Validation RMSE: 0.90842526414\n",
      "\n",
      "Iteration:       1890000\n",
      "Validation RMSE: 0.913009886602\n",
      "\n",
      "Iteration:       1900000\n",
      "Validation RMSE: 0.906363810254\n",
      "\n",
      "Iteration:       1910000\n",
      "Validation RMSE: 0.912684634873\n",
      "\n",
      "Iteration:       1920000\n",
      "Validation RMSE: 0.90563719155\n",
      "Test RMSE      : 0.907740474305\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       1930000\n",
      "Validation RMSE: 0.910410699637\n",
      "\n",
      "Iteration:       1940000\n",
      "Validation RMSE: 0.906060715116\n",
      "\n",
      "Iteration:       1950000\n",
      "Validation RMSE: 0.908309116033\n",
      "\n",
      "Iteration:       1960000\n",
      "Validation RMSE: 0.906882793174\n",
      "\n",
      "Iteration:       1970000\n",
      "Validation RMSE: 0.906700402565\n",
      "\n",
      "Iteration:       1980000\n",
      "Validation RMSE: 0.910921253193\n",
      "\n",
      "Iteration:       1990000\n",
      "Validation RMSE: 0.905399557482\n",
      "Test RMSE      : 0.907323752635\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2000000\n",
      "Validation RMSE: 0.908303407182\n",
      "\n",
      "Iteration:       2010000\n",
      "Validation RMSE: 0.90342703901\n",
      "Test RMSE      : 0.905864774346\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2020000\n",
      "Validation RMSE: 0.907279628492\n",
      "\n",
      "Iteration:       2030000\n",
      "Validation RMSE: 0.903947255264\n",
      "\n",
      "Iteration:       2040000\n",
      "Validation RMSE: 0.90381215701\n",
      "\n",
      "Iteration:       2050000\n",
      "Validation RMSE: 0.902502205992\n",
      "Test RMSE      : 0.904748155547\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2060000\n",
      "Validation RMSE: 0.905521949714\n",
      "\n",
      "Iteration:       2070000\n",
      "Validation RMSE: 0.905139981525\n",
      "\n",
      "Iteration:       2080000\n",
      "Validation RMSE: 0.911708389713\n",
      "\n",
      "Iteration:       2090000\n",
      "Validation RMSE: 0.902362544604\n",
      "Test RMSE      : 0.904522377225\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2100000\n",
      "Validation RMSE: 0.904402212782\n",
      "\n",
      "Iteration:       2110000\n",
      "Validation RMSE: 0.902781371018\n",
      "\n",
      "Iteration:       2120000\n",
      "Validation RMSE: 0.903820925757\n",
      "\n",
      "Iteration:       2130000\n",
      "Validation RMSE: 0.907631309485\n",
      "\n",
      "Iteration:       2140000\n",
      "Validation RMSE: 0.902581788816\n",
      "\n",
      "Iteration:       2150000\n",
      "Validation RMSE: 0.903737591671\n",
      "\n",
      "Iteration:       2160000\n",
      "Validation RMSE: 0.908812557844\n",
      "\n",
      "Iteration:       2170000\n",
      "Validation RMSE: 0.905872061142\n",
      "\n",
      "Iteration:       2180000\n",
      "Validation RMSE: 0.913526886299\n",
      "\n",
      "Iteration:       2190000\n",
      "Validation RMSE: 0.901252183906\n",
      "Test RMSE      : 0.903586914859\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2200000\n",
      "Validation RMSE: 0.901994687577\n",
      "\n",
      "Iteration:       2210000\n",
      "Validation RMSE: 0.901822876004\n",
      "\n",
      "Iteration:       2220000\n",
      "Validation RMSE: 0.902472850818\n",
      "\n",
      "Iteration:       2230000\n",
      "Validation RMSE: 0.904323908207\n",
      "\n",
      "Iteration:       2240000\n",
      "Validation RMSE: 0.913685339546\n",
      "\n",
      "Iteration:       2250000\n",
      "Validation RMSE: 0.900071213134\n",
      "Test RMSE      : 0.90226633285\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2260000\n",
      "Validation RMSE: 0.899781853192\n",
      "Test RMSE      : 0.902005159028\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2270000\n",
      "Validation RMSE: 0.902323105805\n",
      "\n",
      "Iteration:       2280000\n",
      "Validation RMSE: 0.901727604893\n",
      "\n",
      "Iteration:       2290000\n",
      "Validation RMSE: 0.907820547089\n",
      "\n",
      "Iteration:       2300000\n",
      "Validation RMSE: 0.903898439266\n",
      "\n",
      "Iteration:       2310000\n",
      "Validation RMSE: 0.90148286348\n",
      "\n",
      "Iteration:       2320000\n",
      "Validation RMSE: 0.909685320529\n",
      "\n",
      "Iteration:       2330000\n",
      "Validation RMSE: 0.898756945544\n",
      "Test RMSE      : 0.90090655324\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2340000\n",
      "Validation RMSE: 0.899184951664\n",
      "\n",
      "Iteration:       2350000\n",
      "Validation RMSE: 0.898255298496\n",
      "Test RMSE      : 0.900668613128\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2360000\n",
      "Validation RMSE: 0.90812888688\n",
      "\n",
      "Iteration:       2370000\n",
      "Validation RMSE: 0.898158962441\n",
      "Test RMSE      : 0.900256993821\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2380000\n",
      "Validation RMSE: 0.905228021404\n",
      "\n",
      "Iteration:       2390000\n",
      "Validation RMSE: 0.909348566471\n",
      "\n",
      "Iteration:       2400000\n",
      "Validation RMSE: 0.89791144058\n",
      "Test RMSE      : 0.900387384836\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2410000\n",
      "Validation RMSE: 0.898023710505\n",
      "\n",
      "Iteration:       2420000\n",
      "Validation RMSE: 0.899268517763\n",
      "\n",
      "Iteration:       2430000\n",
      "Validation RMSE: 0.898005756511\n",
      "\n",
      "Iteration:       2440000\n",
      "Validation RMSE: 0.89817246617\n",
      "\n",
      "Iteration:       2450000\n",
      "Validation RMSE: 0.91052715448\n",
      "\n",
      "Iteration:       2460000\n",
      "Validation RMSE: 0.900553374284\n",
      "\n",
      "Iteration:       2470000\n",
      "Validation RMSE: 0.898494607309\n",
      "\n",
      "Iteration:       2480000\n",
      "Validation RMSE: 0.898180173122\n",
      "\n",
      "Iteration:       2490000\n",
      "Validation RMSE: 0.900571513329\n",
      "\n",
      "Iteration:       2500000\n",
      "Validation RMSE: 0.897870664822\n",
      "Test RMSE      : 0.899797120232\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2510000\n",
      "Validation RMSE: 0.897601712141\n",
      "Test RMSE      : 0.899553730885\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2520000\n",
      "Validation RMSE: 0.901875910156\n",
      "\n",
      "Iteration:       2530000\n",
      "Validation RMSE: 0.89764755194\n",
      "\n",
      "Iteration:       2540000\n",
      "Validation RMSE: 0.897344142806\n",
      "Test RMSE      : 0.899316075189\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2550000\n",
      "Validation RMSE: 0.897341474397\n",
      "Test RMSE      : 0.899022742027\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2560000\n",
      "Validation RMSE: 0.899353531974\n",
      "\n",
      "Iteration:       2570000\n",
      "Validation RMSE: 0.898951689792\n",
      "\n",
      "Iteration:       2580000\n",
      "Validation RMSE: 0.897635554756\n",
      "\n",
      "Iteration:       2590000\n",
      "Validation RMSE: 0.897781644702\n",
      "\n",
      "Iteration:       2600000\n",
      "Validation RMSE: 0.908447884483\n",
      "\n",
      "Iteration:       2610000\n",
      "Validation RMSE: 0.897190075482\n",
      "Test RMSE      : 0.898968492514\n",
      "Best validation error up to now !\n",
      "\n",
      "Iteration:       2620000\n",
      "Validation RMSE: 0.898445997916\n",
      "\n",
      "Iteration:       2630000\n",
      "Validation RMSE: 0.899599207883\n",
      "\n",
      "Iteration:       2640000\n",
      "Validation RMSE: 0.897568614509\n",
      "\n",
      "Iteration:       2650000\n",
      "Validation RMSE: 0.898357234173\n",
      "\n",
      "Iteration:       2660000\n",
      "Validation RMSE: 0.908477403846\n",
      "\n",
      "Iteration:       2670000\n",
      "Validation RMSE: 0.897823597991\n",
      "\n",
      "Iteration:       2680000\n",
      "Validation RMSE: 0.906850045151\n",
      "\n",
      "Iteration:       2690000\n",
      "Validation RMSE: 0.897699331138\n",
      "\n",
      "Iteration:       2700000\n",
      "Validation RMSE: 0.900099701669\n",
      "\n",
      "Iteration:       2710000\n",
      "Validation RMSE: 0.897403921296\n",
      "\n",
      "Iteration:       2720000\n",
      "Validation RMSE: 0.904877932317\n",
      "\n",
      "Iteration:       2730000\n",
      "Validation RMSE: 0.898519189074\n",
      "\n",
      "Iteration:       2740000\n",
      "Validation RMSE: 0.902453588483\n",
      "\n",
      "Iteration:       2750000\n",
      "Validation RMSE: 0.911427075067\n",
      "\n",
      "Iteration:       2760000\n",
      "Validation RMSE: 0.899624974514\n",
      "\n",
      "Iteration:       2770000\n",
      "Validation RMSE: 0.899841895644\n",
      "\n",
      "Iteration:       2780000\n",
      "Validation RMSE: 0.897706008393\n",
      "\n",
      "Iteration:       2790000\n",
      "Validation RMSE: 0.901481131243\n",
      "\n",
      "Iteration:       2800000\n",
      "Validation RMSE: 0.897551819005\n",
      "\n",
      "Iteration:       2810000\n",
      "Validation RMSE: 0.89809825124\n",
      "Exceed patience for optimization, stopping!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        if sgd_iteration_count%update_frequency == 0:\n",
    "            train_set_rmse = get_trainset_rmse()\n",
    "            valid_set_rmse = get_validset_rmse()\n",
    "            test_set_rmse = get_testset_rmse()\n",
    "            \n",
    "            train_errors.append(train_set_rmse)\n",
    "            valid_errors.append(valid_set_rmse)\n",
    "            test_errors.append(test_set_rmse)\n",
    "            \n",
    "            print 'Iteration:      ', sgd_iteration_count\n",
    "            print 'Validation RMSE:', valid_set_rmse\n",
    "\n",
    "            if valid_set_rmse < best_validation_rmse:\n",
    "                print 'Test RMSE      :', test_set_rmse\n",
    "                print 'Best validation error up to now !'\n",
    "                patience = 0\n",
    "                best_validation_rmse = valid_set_rmse\n",
    "                best_parameters = model.save_parameters()\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= 20:\n",
    "                    print 'Exceed patience for optimization, stopping!'\n",
    "                    break\n",
    "            print\n",
    "        training_idx = sample_random_training_index()\n",
    "        user, item, rating = train_ratings[training_idx]\n",
    "        model.update_parameters(user, item, rating)\n",
    "        sgd_iteration_count += 1\n",
    "    except KeyboardInterrupt:\n",
    "        print 'Stopped Optimization'\n",
    "        print 'Current valid set performance=%s' % compute_rmse(model.predict(*valid_ratings.T[:2]), valid_ratings[:,2])\n",
    "        print 'Current test set performance=%s' % compute_rmse(model.predict(*test_ratings.T[:2]), test_ratings[:,2])\n",
    "        break\n",
    "        \n",
    "model.load_parameters(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much should be wait? How good is the model? Is the optimization ok?\n",
    "\n",
    "### Lets first try to answer some of these questions by looking at the so-called learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x5f69a90>,\n",
       " <matplotlib.lines.Line2D at 0x5f69c50>,\n",
       " <matplotlib.lines.Line2D at 0x6434208>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEECAYAAADpigmnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d3baf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = update_frequency * np.arange(len(train_errors))\n",
    "plt.plot(x, train_errors, 'r--', x, valid_errors, 'bs', x, test_errors, 'g^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that most of the learning is done early during the optimization. However greater results are achieved by waiting.\n",
    "### Training error will always be lower than validation and testing error. When the difference is too great we call this \"overfitting\".\n",
    "### We can achieve better results by using different latent factor size and different learning rates. You will see that bigger latent factor size will lead to more overfitting. Smaller size will overfit less, but may perform more poorly on the test set. The best is somewhere in between...\n",
    "### There are also many other things we can do to improve the performance and \"generalization\":\n",
    "\n",
    "- regularization\n",
    "- learning rate reduction\n",
    "- implicit feedback\n",
    "- tuning all the hyperparameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a model, how do we evaluate it, and more importantly, how do we use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged with product info and fetch most similar movies...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    s1 = s1.lower()\n",
    "    s2 = s2.lower()\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_names_lookup = dict([(movies_info[_id][0].split('(')[0].strip().lower(), _id) for _id in movies_info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(movie_names_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = 'titanic'\n",
    "\n",
    "def get_closest_name(name):\n",
    "    name = name.strip().lower()\n",
    "    ranked = sorted([(n, levenshtein(name, n)) for n in movie_names_lookup], key=lambda x: x[1])\n",
    "    print 'Closest matches:'\n",
    "    closest_matches = [(n, movie_names_lookup[n]) for (n, s) in ranked[:10]]\n",
    "    print closest_matches\n",
    "    return closest_matches[1][1]\n",
    "\n",
    "get_closest_name(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(item):\n",
    "    numerator = (model.V * model.V[item]).sum(axis=-1)\n",
    "    denominator = ( (model.V**2).sum(axis=-1)**0.5 * (model.V[item]**2).sum()**0.5)\n",
    "    return  numerator/denominator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie = 1000\n",
    "print movies_info[movie]\n",
    "\n",
    "cosine_similarity = compute_cosine_similarity(movie)\n",
    "closest = cosine_similarity.argsort()[::-1]\n",
    "\n",
    "# print closest\n",
    "for c in closest[:15]:\n",
    "    print 'Score %s --- %s' % (cosine_similarity[c], movies_info[c])\n",
    "    print \n",
    "    \n",
    "print '###'\n",
    "\n",
    "for c in closest[-15:]:\n",
    "    print 'Score %s --- %s' % (cosine_similarity[c], movies_info[c])\n",
    "    print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_info = open('ml-1m/movies.dat').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_info = dict([(int(m[0]), m[1:]) for m in [m.split('::') for m in movies_info]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "V2d = pca.fit_transform(V)\n",
    "plt.scatter(*V2d.T)\n",
    "plt.show()\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "V2d = tsne.fit_transform(V)\n",
    "plt.scatter(*V2d.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data : basic ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing attributes : encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic problem formulation : matrix factorization/matrix completion, embeddings, learning representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = Ratings[:,-1].mean()\n",
    "bias_u = np.random.normal(0, 0.1, size=N_Users)\n",
    "bias_v = np.random.normal(0, 0.1, size=N_Movies)\n",
    "embedding_size = 50\n",
    "U = np.random.normal(0, 0.1, size=(N_Users, embedding_size))\n",
    "V = np.random.normal(0, 0.1, size=(N_Movies, embedding_size))\n",
    "print len(Users), len(Movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(u, v):\n",
    "    return mu + bias_u[u] + bias_v[v]# + np.dot(U[u], V[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(r, p):\n",
    "    return ((r-p)**2).mean()\n",
    "\n",
    "d_L_d_bu = np.zeros(bias_u.shape)\n",
    "d_L_d_bv = np.zeros(bias_v.shape)\n",
    "\n",
    "def get_gradient(u, v, r):\n",
    "    p = predict(u, v)\n",
    "    d_diff = 2 * (r - p)\n",
    "    user_bias = bias_u[u]\n",
    "    item_bias = bias_v[v]\n",
    "    user_embedding = U[u]\n",
    "    item_embedding = V[v]\n",
    "    d_L_d_mu = -d_diff\n",
    "    d_L_d_bu_tmp = -d_diff * user_bias \n",
    "    d_L_d_bv_tmp = -d_diff * item_bias\n",
    "    \n",
    "    # reset bias gradients\n",
    "    d_L_d_bu = np.zeros(bias_u.shape)\n",
    "    d_L_d_bv = np.zeros(bias_v.shape)\n",
    "    \n",
    "    # sum gradients of each user for their bias\n",
    "    \n",
    "    # Faster way to do it\n",
    "    d_L_d_bu_tmp = pd.DataFrame({'u':u, 'g':d_L_d_bu_tmp}).groupby('u').mean().reset_index()\n",
    "    d_L_d_bv_tmp = pd.DataFrame({'v':v, 'g':d_L_d_bv_tmp}).groupby('v').mean().reset_index()\n",
    "    \n",
    "    d_L_d_bu[d_L_d_bu_tmp['u']] = d_L_d_bu_tmp['g']\n",
    "    d_L_d_bu[d_L_d_bv_tmp['v']] = d_L_d_bv_tmp['g']\n",
    "    \n",
    "    \n",
    "#     for _u in set(u):\n",
    "#         d_L_d_bu[_u] += d_L_d_bu_tmp[u==_u].sum()\n",
    "    \n",
    "#     print 'b'\n",
    "#     # sum gradients of each item for their bias\n",
    "#     for _v in set(v):\n",
    "#         d_L_d_bv[_v] += d_L_d_bv_tmp[v==_v].sum()\n",
    "    \n",
    "    return {\"mu\":d_L_d_mu.mean(), \"bu\":d_L_d_bu, \"bv\":d_L_d_bv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = predict(*Ratings.T[:-1])\n",
    "print loss(Ratings.T[-1], p)\n",
    "g = get_gradient(*Ratings.T)\n",
    "\n",
    "lr = 1\n",
    "\n",
    "mu = mu - lr * g['mu']#.sum()#.mean()\n",
    "bias_u = bias_u - lr * g['bu']\n",
    "bias_v = bias_v - lr * g['bv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doesnt work!! still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g['bu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bias_v[Ratings.T[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization framework & options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [float(x.strip()) for x in open('C:/Users/1020384/Desktop/ratio_values.csv').readlines()]\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x[x>2]\n",
    "plt.hist(x[x<2], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ratings = {}\n",
    "item_ratings = {}\n",
    "user_rated_items = {}\n",
    "item_rating_users = {}\n",
    "\n",
    "for u, i, r in train_ratings:\n",
    "    if u not in user_ratings:\n",
    "        user_ratings[u] = {}\n",
    "    if i not in item_ratings:\n",
    "        item_ratings[i] = {}\n",
    "    user_ratings[u][i] = r\n",
    "    item_ratings[i][u] = r\n",
    "    \n",
    "    if u not in user_rated_items:\n",
    "        user_rated_items[u] = set()\n",
    "    if i not in item_rating_users:\n",
    "        item_rating_users[i] = set()\n",
    "    user_rated_items[u].add(i)\n",
    "    item_rating_users[i].add(u)\n",
    "\n",
    "user, item, rating = valid_ratings[0]\n",
    "\n",
    "# compute similarity of all other items for that item - itemwise because items are rated by alot of users\n",
    "\n",
    "similarity = {}\n",
    "for item_i in (set(item_rating_users.keys()) - set([item])):\n",
    "    \n",
    "    i_ratings = item_ratings[item_i]\n",
    "\n",
    "    # Users that rated both \"item\" and \"i\"\n",
    "    common_users = item_rating_users[item] & item_rating_users[item_i]\n",
    "    \n",
    "    # Compute similarity between user and each of the common users.\n",
    "    current_user_ratings = user_ratings[user]\n",
    "    \n",
    "    import pdb; pdb.set_trace()\n",
    "    \n",
    "    print common_users"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
